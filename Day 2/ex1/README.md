
# Sales Data ETL Pipeline

This project is an ETL (Extract, Transform, Load) pipeline to process sales data from a CSV file.

## ğŸ“ Project Structure

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ sales.csv            # Input raw CSV file containing sales data
â”‚   â””â”€â”€ output/
â”‚       â””â”€â”€ processed_sales.csv  # Output processed CSV file containing filtered and transformed data
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ etl_pipeline.py          # Main script for the ETL pipeline logic
â”‚   â””â”€â”€ config.py                # Configuration file (e.g., settings, paths)
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_etl_pipeline.py     # Unit tests to verify the correctness of the ETL pipeline
â”‚
â”œâ”€â”€ requirements.txt             # Python dependencies for the project
â””â”€â”€ README.md
```

## â–¶ï¸ How to Run

1. Install dependencies:

```bash
pip install -r requirements.txt
```

2. Run the ETL pipeline:

```bash
python src/etl_pipeline.py
```

## âœ… What it does

- Reads raw sales data from `data/raw/sales.csv`
- Filters rows where `Amount > 100`
- Aggregates total sales amount
- Writes the result to `data/output/processed_sales.csv`


## ğŸ³ Run with Docker

To build and run the pipeline using Docker:

```bash
docker-compose up --build
```

This will:
- Build a Docker image using the `Dockerfile`
- Mount the `data/` folder so the processed result is saved on your local filesystem
